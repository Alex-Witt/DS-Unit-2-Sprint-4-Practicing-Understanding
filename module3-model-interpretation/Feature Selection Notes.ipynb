{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Noise:  (0.7182483686213841, 7.32401731299835e-49)\n",
      "Higher Noise:  (0.057964292079338155, 0.3170099388532475)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "np.random.seed(0)\n",
    "\n",
    "size = 300\n",
    "\n",
    "x = np.random.normal(0,1, size)\n",
    "\n",
    "print('Lower Noise: ', pearsonr(x, x + np.random.normal(0, 1, size)))\n",
    "print('Higher Noise: ', pearsonr(x, x + np.random.normal(0, 10, size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As you can see from the example, we compare a variable with a noisy version of itself. With smaller amount of noise, the correlation is relatively strong, with a very low p-value, while for the noisy comparison, the correlation is very small and furthermore, the p-value is high meaning that it is very likely to observe such correlation on a dataset of this size purely by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Noise:  (0.6857150036658147, 7.860081779159927e-140)\n",
      "Higher Noise:  (0.12083574866694574, 0.00012792152128382383)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "np.random.seed(42)\n",
    "\n",
    "size = 1000\n",
    "\n",
    "x = np.random.normal(0,1, size)\n",
    "\n",
    "print('Lower Noise: ', pearsonr(x, x + np.random.normal(0, 1, size)))\n",
    "print('Higher Noise: ', pearsonr(x, x + np.random.normal(0, 10, size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Similarly, in this example the correlation remains relative but in the noisy comparison, the p-value is small meaning that it is unlikely to observe such a correlation by chance. \n",
    "\n",
    ">Scikit-learn provides f_regression method for computing the p-values (and underlying F-values) in batch for a set of features, so it is a convenient way to run a corre1ation test on a dataset in one go and for example include it in a sklearn’s pipeline.\n",
    "\n",
    ">One obvious drawback of Pearson correlation as a feature ranking mechanism is that it is only sensitive to a linear relationship. If the relation is non-linear, Pearson correlation can be close to zero even if there is a 1-1 correspondence between the two variables.\n",
    "For example, correlation between x and x2 is zero, when x is centered on 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.004932875787130827\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(-1, 1, 100000)\n",
    "print (pearsonr(x, x**2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information and maximal information coefficient (MIC)\n",
    "\n",
    "MIC measures mutual dependence between variables, typically in bits. Threfeore the data ** is not ** normalized. Therefore it can be difficult to use in certain circumstances. \n",
    "\n",
    ">- MI Values can be incomparable between two datasets. \n",
    ">- It can be difficult to compute for continuous variables. They need to be discrete, and if continueous, binned. However, it can be sensitive to bin selection. \n",
    "\n",
    "Therefore another technique was developed to address the shortcomings. \n",
    "\n",
    "\n",
    "### Maximal information Coefficient (MINE)\n",
    "\n",
    "The Maximal information coefficent searches for optimal binning and turns mutual information score into a metric that lies in range (0,1). It is available via the minepy library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'minepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-dc4a6af25433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## INSTALL MINEPY AT THE NEXT AIRPORT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mminepy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'minepy'"
     ]
    }
   ],
   "source": [
    "## INSTALL MINEPY AT THE NEXT AIRPORT\n",
    "\n",
    "from minepy import MINE\n",
    "\n",
    "m = MINE()\n",
    "X = np.random.uniform(-1,1,10000)\n",
    "\n",
    "print(m.mic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the correlation we were looking for. \n",
    "\n",
    "Further reading on MIC's statistical power: \n",
    "> http://ie.technion.ac.il/~gorfinm/files/science6.pdf\n",
    "> http://www-stat.stanford.edu/~tibs/reshef/comment.pdf\n",
    "> http://en.wikipedia.org/wiki/Statistical_power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Correlation\n",
    "\n",
    ">Another robust, competing method of correlation estimation is distance correlation, designed explicitly to address the shortcomings of Pearson correlation. While for Pearson correlation, the correlation value 0 does not imply independence (as we saw from the x vs x2 example), distance correlation of 0 does imply that there is no dependence between the two variables.\n",
    "\n",
    "[python gist](https://gist.github.com/josef-pkt/2938402)\n",
    "\n",
    "> There are at least two reasons why to prefer Pearson correlation over more sophisticated methods such as MIC or distance correlation when the relationship is close to linear. For one, computing the Pearson correlation is much faster, which may be important in case of big datasets. Secondly, the range of the correlation coefficient is [-1;1] (instead of [0;1] for MIC and distance correlation). This can relay useful extra information on whether the relationship is negative or positive, i.e. do higher feature values imply higher values of the response variables or vice versa. But of course the question of negative versus positive correlation is only well-posed if the relationship between the two variables is monotonic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Based Ranking\n",
    "\n",
    ">Finally one can use an arbitrary machine learning method to build a predictive model for the response variable using each individual feature, and measure the performance of each model. **In fact, this is already put to use with Pearson’s correlation coefficient, since it is equivalent to standardized regression coefficient that is used for prediction in linear regression.** \n",
    "\n",
    ">If the relationship between a feature and the response variable is non-linear, there are a number of alternatives, for example tree based methods (decision trees, random forest), linear model with basis expansion etc. Tree based methods are probably among the easiest to apply, since they can model non-linear relations well and don’t require much tuning. The main thing to avoid is overfitting, so the depth of tree(s) should be relatively small, and cross-validation should be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-2.378, 'DIS'), (-2.6, 'LSTAT'), (-2.727, 'PTRATIO'), (-2.977, 'RM'), (-3.299, 'TAX'), (-4.422, 'CRIM'), (-4.827, 'ZN'), (-4.843, 'INDUS'), (-6.667, 'B'), (-6.948, 'RAD'), (-8.123, 'AGE'), (-8.771, 'NOX'), (-12.315, 'CHAS')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Boston Housing Data\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "names = boston['feature_names']\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 20, max_depth = 4)\n",
    "scores = []\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    score = cross_val_score(model, X[:, i:i+1], y, scoring = 'r2', \n",
    "                            cv = ShuffleSplit(len(X), 3, .3))\n",
    "    scores.append((round(np.mean(score), 3), names[i]))\n",
    "    \n",
    "print (sorted (scores, reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
